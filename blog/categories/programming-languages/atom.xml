<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: programming languages | Simple Solutions]]></title>
  <link href="http://bartoszsypytkowski.com/blog/categories/programming-languages/atom.xml" rel="self"/>
  <link href="http://bartoszsypytkowski.com/"/>
  <updated>2015-03-14T13:19:00+01:00</updated>
  <id>http://bartoszsypytkowski.com/</id>
  <author>
    <name><![CDATA[Bartosz Sypytkowski]]></name>
    <email><![CDATA[b.sypytkowski@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Simple Virtual Machine]]></title>
    <link href="http://bartoszsypytkowski.com/blog/2014/08/02/simple-vm/"/>
    <updated>2014-08-02T13:21:00+02:00</updated>
    <id>http://bartoszsypytkowski.com/blog/2014/08/02/simple-vm</id>
    <content type="html"><![CDATA[<p>In this post I want to present, how to create a simple virtual machine with it&rsquo;s own bytecode interpreter. While it&rsquo;s created using C language, it doesn&rsquo;t use any complex constructs and could be possibly implemented the same with any imperative language.</p>

<p>NOTE: while in this example bytecode instruction set is limited to the simple integer operations, real life virtual machines usually have a lot more complex operands (eg. object creation instruction in VMs with managed memory or dynamic method invocations). Have in mind that even if your Intermediate Representation may look like some BASIC/ASM language, it really could abstract much higher level functionalities.</p>

<h3>Virtual Machine</h3>

<p>Lets start with precising, what our virtual machine should consist of. In this example, we&rsquo;ll create a stack-based virtual machine. What does it mean? There is no notion of registers and all operation are performed through virtual stack. This results in simpler (since our instruction set doesn&rsquo;t have to include virtual registers operations), but also longer bytecode &ndash; because of more operations have to be executed than in alternative option, which may support a virtual register calls.</p>

<p>In this example, our virtual machine will consist of:</p>

<ul>
<li><strong>Executable code pointer</strong> &ndash; in order to execute the program, a VM has to now what program it is ;) Our bytecode contains opcodes (which are integers used to define instructions to be executed by an interpreter) and numbers used for things such as constant values, memory addresses or stack offsets.</li>
<li><strong>Stack</strong> &ndash; virtual stack will work as fragment of memory used for storing intermediate results and additional data of each instruction. When a procedure call will be invoked, it will also save a state of the program at moment, when procedure has been called, to retrieve it back after program will return from that call.</li>
<li><strong>Local data</strong> &ndash; unlike the stack, which uses LIFO (Last-In First-Out) operations, local data may be used as random access memory. It also may change it&rsquo;s size according to program needs. In this example we may use it to store both global and local variables.</li>
<li><strong>Program counter</strong> &ndash; it contains an address of currently executed instruction. Program will be executed by moving PC through instruction set, reading opcodes and executing them.</li>
<li><strong>Stack pointer</strong> &ndash; it contains information about number of elements stored on the stack. It always points on the top of it.</li>
<li><strong>Frame pointer</strong> &ndash; since we need to differentiate a locally and globally scoped values (this includes function arguments), we need to know where our scope is located on the stack. All local variables are assigned relatively to FP position.</li>
</ul>


<p>```c
// stack will have fixed size</p>

<h1>define STACK_SIZE 100</h1>

<p>typedef struct {</p>

<pre><code>int* locals;    // local scoped data
int* code;      // array od byte codes to be executed
int* stack;     // virtual stack
int pc;         // program counter (aka. IP - instruction pointer)
int sp;         // stack pointer
int fp;         // frame pointer (for local scope)
</code></pre>

<p>} VM;</p>

<p>VM<em> newVM(int</em> code,    // pointer to table containing a bytecode to be executed</p>

<pre><code>int pc,             // address of instruction to be invoked as first one - entrypoint/main func
int datasize){      // total locals size required to perform a program operations
VM* vm = (VM*)malloc(sizeof(VM));
vm-&gt;code = code;
vm-&gt;pc = pc;
vm-&gt;fp = 0;
vm-&gt;sp = -1;
vm-&gt;locals = (int*)malloc(sizeof(int) * datasize);
vm-&gt;stack = (int*)malloc(sizeof(int) * STACK_SIZE);

return vm;
</code></pre>

<p>}</p>

<p>void delVM(VM* vm){</p>

<pre><code>free(vm-&gt;locals);
free(vm-&gt;stack);
free(vm);
</code></pre>

<p>}
```</p>

<h3>Bytecode instruction set</h3>

<p>At the begginig we&rsquo;ll start from defining a set of instructions to be handled by our language. For sake of simplicity we&rsquo;ll focus on basic integer operations, printing their result, variable creation and procedure-oriented calls.</p>

<p>```c
enum {</p>

<pre><code>ADD_I32 = 1,    // int add
SUB_I32 = 2,    // int sub
MUL_I32 = 3,    // int mul
LT_I32 = 4,     // int less than
EQ_I32 = 5,     // int equal
JMP = 6,        // branch
JMPT = 7,       // branch if true
JMPF = 8,       // branch if false
CONST_I32 = 9,  // push constant integer
LOAD = 10,      // load from local
GLOAD = 11,     // load from global
STORE = 12,     // store in local
GSTORE = 13,    // store in global memory
PRINT = 14,     // print value on top of the stack
POP = 15,       // throw away top of the stack
HALT = 16,      // stop program
CALL = 17,      // call procedure
RET = 18        // return from procedure
</code></pre>

<p>};
```
Just like in standard assembly code, each instruction is represented by specific byte number. In more realistic virtual machine those numbers may have specific values (for example to perform bitwise masking).</p>

<h3>Main interpreter loop</h3>

<p>It&rsquo;s time to create our code execution loop. How does it work? It&rsquo;s a simple loop with switch statement used to interpret particular operation codes found in our program. While this is not the fastest solution, it gives us a simple understanding of how fetch/decode/execute cycle works.</p>

<p>```c</p>

<h1>define PUSH(vm, v) vm->stack[++vm->sp] = v // push value on top of the stack</h1>

<h1>define POP(vm)     vm->stack[vm->sp&mdash;]     // pop value from top of the stack</h1>

<h1>define NCODE(vm)   vm->code[vm->pc++]      // get next bytecode</h1>

<p>void run(VM* vm){</p>

<pre><code>do{
    int opcode = NCODE(vm);        // fetch
    int v, addr, offset, a, b, argc, rval;

    switch (opcode) {   // decode
    case HALT: return;  // stop the program
    case CONST_I32: ...
    case ADD_I32: ...
    case SUB_I32: ...
    case MUL_I32: ...
    case LT_I32: ...
    case EQ_I32: ...
    case JMP: ...
    case JMPT: ...
    case JMPF: ...
    case LOAD:  ...
    case GLOAD: ...
    case GSTORE: ...
    case CALL: ...
    case RET: ...
    case POP: 
        --vm-&gt;sp;      // throw away value at top of the stack
        break;
    case PRINT: 
        v = POP(vm);        // pop value from top of the stack ...
        printf("%d\n", v);  // ... and print it
        break;
    default:
        break;
    }

}while(1);
</code></pre>

<p>}
<code>``
Here we've already defined</code>HALT<code>,</code>POP<code>and</code>PRINT` opcodes. In more realistic example, you probably don&rsquo;t want to define value display in form of dedicated instruction, but this will allow us to simply see result of our operations.</p>

<h3>Integer operators</h3>

<p>Now we can define some integer operations. While we&rsquo;re defined a plenty of them, here I&rsquo;ll show only one example for each group: constant initialization, arithmetic and logic operations.</p>

<p>What is worth emphasizing, is that all value swaps are performed through stack (there are no registers), so each instruction takes it&rsquo;s operands from top of the stack and also puts a computed result on top of it. Also, since stacks are LIFO-type collections, all values taken from the stack are in reversed order &ndash; for example look at <code>ADD_I32</code> instruction below.</p>

<p>```c
case CONST_I32:</p>

<pre><code>v = NCODE(vm);   // get next value from code ...
PUSH(vm, v);     // ... and move it on top of the stack
break;
</code></pre>

<p>case ADD_I32:</p>

<pre><code>b = POP(vm);        // get second value from top of the stack ...
a = POP(vm);        // ... then get first value from top of the stack ...
PUSH(vm, a + b);    // ... add those two values and put result on top of the stack
break;
</code></pre>

<p>case LT_I32:</p>

<pre><code>b = POP(vm);        // get second value from top of the stack ...
a = POP(vm);        // ... then get first value from top of the stack ...
PUSH(vm, (a&lt;b) ? 1 : 0); // ... compare those two values, and put result on top of the stack
break;
</code></pre>

<p><code>``
Instructions such as</code>SUB_I32<code>,</code>MUL_I32<code>and</code>EQ_I32` won&rsquo;t be defined in scope of this post, but I think when you&rsquo;ll understand example above, they should be easy to implement.</p>

<h3>Jump instructions</h3>

<p>Next, we have to define jump instructions in our code. They are basically equivalent of <code>goto</code> keyword and may be used to implement statements such as loops and if/else conditions.</p>

<p>```c
case JMP:</p>

<pre><code>vm-&gt;pc = NCODE(vm);  // unconditionaly jump with program counter to provided address
break;
</code></pre>

<p>case JMPT:</p>

<pre><code>addr = NCODE(vm);  // get address pointer from code ...
if(POP(vm)) {      // ... pop value from top of the stack, and if it's true ...
    vm-&gt;pc = addr; // ... jump with program counter to provided address
    }
break;
</code></pre>

<p><code>``
Again,</code>JUMPF<code>(conditional jump if false) implementation should be analogous to</code>JMPT`.</p>

<h3>Move memory globally</h3>

<p>Now, when all basic constructs are defined, we shall provide a way to define a variables in our program. This paragraph will cover problem of globally-scoped ones, while the next one will show how to implement locally-scoped variables.</p>

<p>Unlike stack, a local data is a random access memory of elastic size, so all in/out operations performed on it using memory addresses (aka. pointers). Therefore in our example all load/store opcodes should be followed by number being an address pointer to the local data.</p>

<p>```c
case GLOAD:</p>

<pre><code>addr = POP(vm);             // get pointer address from code ...
v = vm-&gt;locals[addr];         // ... load value from memory of the provided addres ...
PUSH(vm, v);                // ... and put that value on top of the stack
break;
</code></pre>

<p>case GSTORE:</p>

<pre><code>v = POP(vm);                // get value from top of the stack ...
addr = NCODE(vm);           // ... get pointer address from code ...
vm-&gt;locals[addr] = v;         // ... and store value at address received
break;
</code></pre>

<p>```</p>

<h3>Manipulate local scope memory</h3>

<p>This is similar to previously defined <code>GLOAD</code> and <code>GSTORE</code> instruction. However there is a one significant difference &ndash; instead of taking an locals address directly from provided value, we compute it relatively to current frame pointer.
```c
case LOAD:                  // load local value or function arg</p>

<pre><code>offset = NCODE(vm);     // get next value from code to identify local variables offset start on the stack
PUSH(vm, vm-&gt;stack[vm-&gt;fp+offset]); // ... put on the top of the stack variable stored relatively to frame pointer
break;
</code></pre>

<p>case STORE:                 // store local value or function arg</p>

<pre><code>v = POP(vm);            // get value from top of the stack ...
offset = NCODE(vm);     // ... get the relative pointer address from code ...
vm-&gt;locals[vm-&gt;fp+offset] = v;  // ... and store value at address received relatively to frame pointer
break;
</code></pre>

<p>```</p>

<h3>Procedure calls</h3>

<p>All of previous instructions were fairly simple to execute. Now it&rsquo;s time to do something slightly more complex &ndash; function/procedure calls. Before we begin, lets describe how they works. To perform the call, we have to first save the current state of the program at the moment before the call will be performed &ndash; it will let us freely alter program state in scope of the function. To do so, we have to save three values:</p>

<ul>
<li>Address of the caller instruction &ndash; this is a current value of program counter and it&rsquo;s need to be stored, so that VM will know, where program should return from finished procedure call.</li>
<li>Frame pointer &ndash; this is necessary, since we want to be able to use locally scoped variables when we enter into new procedure (<strong>tl;dr</strong> &ndash; it gives us a function scope feature).</li>
<li>Number of function/procedure arguments &ndash; before call will be performed, all necessary arguments will be put on top of the stack. However returning from function should dispose all of the provided arguments, so that we could retrieve back state of the stack from before the function call. If we want to know how many arguments from the stack should be thrown away, firstly we have to store that count &ndash; it will be also put on top of the stack.</li>
</ul>


<p><code>RET</code> instruction works in reverse order &ndash; it takes value from top of the stack as function return value, rollbacks frame pointers, moves program counter to the caller instruction, disposes all of function call arguments from the stack and replaces all of these with return value.</p>

<p>Because we&rsquo;ve stored all of those values on top of the stack, we&rsquo;re able to store next consecutive calls in the same manner. It means that, we&rsquo;re able to nest our procedure calls. However, since each call stores some portion of data on the stack, at some point we may run out of memory, which cause stack overflow error.
```c
case CALL:</p>

<pre><code>// we expect all args to be on the stack
addr = NCODE(vm); // get next instruction as an address of procedure jump ...
argc = NCODE(vm); // ... and next one as number of arguments to load ...
PUSH(vm, argc);   // ... save num args ...
PUSH(vm, vm-&gt;fp); // ... save function pointer ...
PUSH(vm, vm-&gt;pc); // ... save instruction pointer ...
vm-&gt;fp = vm-&gt;sp;  // ... set new frame pointer ...
vm-&gt;pc = addr;    // ... move instruction pointer to target procedure address
break;
</code></pre>

<p>case RET:</p>

<pre><code>rval = POP(vm);     // pop return value from top of the stack
vm-&gt;sp = vm-&gt;fp;    // ... return from procedure address ...
vm-&gt;pc = POP(vm);   // ... restore instruction pointer ...
vm-&gt;fp = POP(vm);   // ... restore framepointer ...
argc = POP(vm);     // ... hom many args procedure has ...
vm-&gt;sp -= argc;     // ... discard all of the args left ...
PUSH(vm, rval);     // ... leave return value on top of the stack
break;
</code></pre>

<p>```</p>

<h3>Example code</h3>

<p>Now, when our bytecode interpreter is finally complete, we may test it on some example program. Please note numbers in comments on the right &ndash; they show actual opcode instruction address, and will be used for conditional jumps and procedure calls.</p>

<p>```c
const int fib = 0;  // address of the fibonacci procedure
int program[] = {</p>

<pre><code>// int fib(n) {
//     if(n == 0) return 0;
LOAD, -3,       // 0 - load last function argument N
CONST_I32, 0,   // 2 - put 0
EQ_I32,         // 4 - check equality: N == 0
JMPF, 10,       // 5 - if they are NOT equal, goto 10
CONST_I32, 0,   // 7 - otherwise put 0
RET,            // 9 - and return it
//     if(n &lt; 3) return 1;
LOAD, -3,       // 10 - load last function argument N
CONST_I32, 3,   // 12 - put 3
LT_I32,         // 14 - check if 3 is less than N
JMPF, 20,       // 15 - if 3 is NOT less than N, goto 20
CONST_I32, 1,   // 17 - otherwise put 1
RET,            // 19 - and return it
//     else return fib(n-1) + fib(n-2);
LOAD, -3,       // 20 - load last function argument N
CONST_I32, 1,   // 22 - put 1
SUB_I32,        // 24 - calculate: N-1, result is on the stack
CALL, fib, 1,   // 25 - call fib function with 1 arg. from the stack
LOAD, -3,       // 28 - load N again
CONST_I32, 2,   // 30 - put 2
SUB_I32,        // 32 - calculate: N-2, result is on the stack
CALL, fib, 1,   // 33 - call fib function with 1 arg. from the stack
ADD_I32,        // 36 - since 2 fibs pushed their ret values on the stack, just add them
RET,            // 37 - return from procedure
// entrypoint - main function
CONST_I32, 6,   // 38 - put 6 
CALL, fib, 1,   // 40 - call function: fib(arg) where arg = 6;
PRINT,          // 43 - print result
HALT            // 44 - stop program
</code></pre>

<p>};</p>

<p>// initialize virtual machine
VM* vm = newVM(program,   // program to execute</p>

<pre><code>               38,    // start address of main function
               0);    // locals to be reserved, fib doesn't require them
</code></pre>

<p>run(vm);
```</p>

<p>As you may have seen, there is a lot of <code>LOAD, -3</code> operations. Why -3? Again, when we perform a procedure call, we need to store current program state on top of the stack. To do so, we&rsquo;ll put here a 3 values: program counter (as return address), frame pointers before the call, and number of arguments to be used by called procedure. Therefore everything below them (-3 and less) will be an actual function arguments saved on the stack. You could optimize it by adding something like <code>LDARG, x</code> opcode.</p>

<h3>Why?</h3>

<p>There are numerous reasons, why developers may want to use intermediate bytecode representations for their languages:</p>

<ul>
<li><strong>Performance</strong> &ndash; as you&rsquo;ve seen, execution of the bytecode is realized mostly by just moving (mostly incrementing) PC pointer. When interpreter works directly by evaluating and traversing an AST tree, there is a lot of pointer jumping and structure blocks, which is slightly more complex and slower. Bytecode allows you to faster code interpretation as well as it may facilitate some optimizations.</li>
<li><strong>Code size</strong> &ndash; full code operates on human readable strings, which may consume some memory and are harder to interpret. Intermediate program representation uses only a simple integer arrays.</li>
<li><strong>Well-fitting</strong> &ndash; since you may describe your own IR, you may fit it directly to your needs. It&rsquo;s easier to parse, and it could remove some repetitive work, when you&rsquo;ll want to implement your language on the various OSes.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Value absence - why Scala/F# approach is bad?]]></title>
    <link href="http://bartoszsypytkowski.com/blog/2014/07/03/fsharp-and-scala-absent-values/"/>
    <updated>2014-07-03T00:05:00+02:00</updated>
    <id>http://bartoszsypytkowski.com/blog/2014/07/03/fsharp-and-scala-absent-values</id>
    <content type="html"><![CDATA[<p>Handling an absent values is one of the most common problems in programming languages. Why we consider this so important? It&rsquo;s because data processing is essential task of every program ever made. And one of the main reasons of systems errors and undefined behaviors are values not present in the system. Most of the modern programming languages tries to aim programmers with solution to handle those cases. Lets look at the most common way of dealing with them:</p>

<ol>
<li><strong>Null pointer reference</strong> &ndash; also called a &ldquo;billion-dollar mistake&rdquo;, introduced by Tony Hoare. It allows to represent an absence of the heap allocated values (objects) in form of pointer referring to not a valid instance of represented type. In most of the mainstream programming languages all heap objects are nullable by default &ndash; no matter if you want it or not.</li>
<li><strong>Option/Maybe type</strong> &ndash; this approach is characteristic for most of the functional languages. Instead of creating null pointer, it defines a special generic type (eg. Option), represented by either one of it&rsquo;s subtypes (Some as wrapper for existing underlying value, or None if value is not present).</li>
</ol>


<p>So why we consider a dedicated Option type a better solution? Because it&rsquo;s explicit, propagates type information about value nullability and &ndash; most of all &ndash; it&rsquo;s non-default. By the most of the time, we expect to have a meaningful references to our data. Their absence is not a desired and certainly should not be an expected behavior. Problem in nullable references is that literally any instance in our code could represent a non-existing value. Analogously to functional approach, all of our objects always represent an Option type.</p>

<p>Currently, after decades on living in shadows, functional paradigm is gaining more and more ground of the field of mainstream programming languages. This also reflects in two of the most popular programming environments, JVM and .NET. Their functional representatives, Scala and F#, claims to combine best of both programming paradigms. This way they also introduced and popularized an Option types among object oriented approach.</p>

<p>But something has been missed along the way, and (in my opinion) both Scala and F# had failed in face of the problem of value absence. Why? Because of their dualism. Since they are functional languages, both of them allows you to create an Option types with full support expected from a functional language. But while they also are object oriented and built on top of object oriented VMs, they allow you to use and create a nullable references. This leads to false sense of security, when using Option types. Lets look at the following example (F#):
<code>fsharp
let value: String option = Some doSmthAndReturnString()
</code></p>

<p>How can you be sure that function <code>doSmthAndReturnString</code> won&rsquo;t return a null? Actually you can&rsquo;t until you perform an explicit null check. From the runtime perspective <code>Some null</code> is a perfectly valid record. Does it sound rational? No. So why is this even possible?</p>

<h2>How to combine functional and object oriented worlds?</h2>

<p>From my perspective, the best solution of that problem came with <a href="http://ceylon-lang.org/">Ceylon</a> language. It&rsquo;s based on the concept of <a href="http://ceylon-lang.org/documentation/1.0/tour/types/#union_types">Union Types</a>, one of the key features of Ceylon. For <strong>tl;dr</strong> people &ndash; union type of <code>X|Y</code> is a supertype of both types <code>X</code> and <code>Y</code>. How does it correspond to null/options?</p>

<p>In Ceylon <code>null</code> have it&rsquo;s own unique type <code>Null</code> (just like in Scala). By default all reference types are non-nullable (similarly to functional languages). However they may be nulled if referenced as union type of <code>T|Null</code> (or <code>T?</code> using some syntax suggar). This concept is basically &ndash; however not entirely &ndash; equivalent of Scala <code>Either[T,Null]</code> but without verbose requirement for object wrapping. Moreover, Ceylon compiler is able to optimize this away to standard JVM null references, without any overhead.
```
// Scala &ndash; Either
var m: Either[String, Null] = Left(&ldquo;hello&rdquo;)
var n: Either[String, Null] = Right(null)<br/>
var m: Either[String, Null] = Left(null)    // still valid</p>

<p>// Scala &ndash; Option type
var m: Option[String] = Some(&ldquo;hello&rdquo;)
var n: Option[String] = None <br/>
var m: Option[String] = Some(null)          // still valid</p>

<p>// Ceylon
String|Null m = &ldquo;hello&rdquo;;    // no need for value wrapping
String? n = null;           // Integer? &ndash;> Integer|Null
String f = null;            // compile time error, references are not nullable by default
```
As you can see on the example above, this way we combined advantages of safe, explicitly nullable types with verbosity and performance of null referenced types. In my opinion this is a proper, yet still not widespread, solution of one of the most common problem in languages theory.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Top features every statically typed language should have]]></title>
    <link href="http://bartoszsypytkowski.com/blog/2014/02/02/uncommon-static-type-feats/"/>
    <updated>2014-02-02T15:29:00+01:00</updated>
    <id>http://bartoszsypytkowski.com/blog/2014/02/02/uncommon-static-type-feats</id>
    <content type="html"><![CDATA[<p>In this post I want to focus on features of statically typed languages which I consider interesting and usefull, but still uncommon and rare among most popular ones in dev environment. First of all, lets try to answer the question: why static typing? We need to realize, what advantages of static typing do we embrace, so we can look for further improvements.</p>

<ol>
<li>Performance &ndash; of course this is first notable difference. Since static typing gives a compiler much more informations about generated program, we have both much heavier optimizations that could be done at compile time and less need of type checking at runtime.</li>
<li>Better tooling &ndash; if our compiler can reason more about our code, so do other tools we use in application development.</li>
<li>Program correctness &ndash; we don&rsquo;t have to be constantly aware of all application functionalities. Because we have expert system in the form of a compiler, we can maintain larger code bases.</li>
</ol>


<p>In scope of this post I want to notice possible features, which may enhance both programmer productivity, application logic correctness and overal code design.</p>

<h2>Type unions and intersections</h2>

<p>Languages: <a href="http://ceylon-lang.org/">Ceylon</a></p>

<p>This feature gives us a profit of union and intersection operations as we know them in set theory, and apply them to types in our language. Type and set theories are very closely related, and they both can benefit with many of common theorems (for example: roots of Hindley-Milner type inference lies in set theory).</p>

<p>Type unions seems to be more natural way to represent some relations, than inheritance. Example: think about type, which could represent all numbers, lets call it <code>Number</code>. From type theory it looks like the best fit would be (literally) union of integer and floating point numbers, which give us a representiation of most of the domain. If this isn&rsquo;t sufficient, it&rsquo;s even easier to extend our Number type with union of more than two types. Now try to do it only by using inheritance or composition&hellip;</p>

<p>Type unions allows us to do the most important thing, most of the non-functional languages do wrong &ndash; proper null handling. In Ceylon <code>Null</code> have it&rsquo;s own type (however it&rsquo;s optimized away during compilation) and is subject to all mentioned rules. All reference variables and field are non-nullable by default &ndash; thanks to unions this behavior could be changed by using notation <code>T|Null</code> or it&rsquo;s simplified form <code>T?</code> for variable type declaration. It&rsquo;s actually a good thing, since this is what we expect from variables in our code for 90% of the time. So why we have to code in continous risk of null exceptions (unless we perform explicit checking), if we may tell compiler to ensure that for us before our code even run? On the other hand, there are a nullable value types, which are also quite usefull at some times. Many languages allow us to use some kind of built-in nullable wrappers, sorrounded with specific compiler magic for programmer convinience (eg. C#). But in my opinion this is not the right way of doing things. If compiler is allowed to use some nice tricks, then we as the programmers also should be able to do so.</p>

<h2>Type constraints / Value dependent types</h2>

<p>Languages: Ada, Eiffel, (experimentally through extensions) all ML-familly languages including Haskell, F*, <a href="http://www.idris-lang.org/">Idris</a></p>

<p>Since we already have a unions and intersections, it would be nice to allow other operations to be performed directly on types. What about conditions?</p>

<p>Lets say I&rsquo;ve got an <code>int</code> type, so I could verify at compile time, that all of the necessary values are restricted to be integers. Ok. Lets go further. What if I want to narrow those integers to omit negative numbers? Actually this is still easy in most of the languages (Java, you have one job&hellip;), since they already have some kind of <code>unsigned int</code> type. But what when I want to go a step further: I want to ensure that my integers are only positive numbers, that means they&rsquo;re neither negatives nor 0. It should be easy, but it&rsquo;s not. Almost all of the languages fail at that moment, requiring programmer to manually check that condition through code at runtime. And this is where type constraints comes to play.</p>

<p>They allow programmer to define specific constraints on values, they refer to. Let me precise this on some pseudocode examples:
<code>
type Positive = x:int where x &gt; 0
type Even = x:int where x % 2 == 0
type IPv4 = x:unsigned byte[] where x.Length == 4
</code>
So why I consider this nice and usefull? Some languages (eg. C/C++, Go) already offers fixed length arrays, while equivalent the example <code>Even</code> type could be created as specialized class? First of all it&rsquo;s easier, since you have one way to solve multiple problems. It also could be implemented fast (think about implementing and working with custom Even class in language such as Java), which makes it more likely to be used in real life. Next, it&rsquo;s all available at compiler level, when you can actually verify your code before running it and allow compiler for further optimizations. They also could be potentialy empovered when used with type set-like operations described in previous paragraph. And finally refinement types could also be used for implementing Design by Contract principles in your code, which by itself is a nice feature.</p>

<h2>Implicit interfaces</h2>

<p>Languages: <a href="http://golang.org/">Go</a>, <a href="http://www.typescriptlang.org/">TypeScript</a></p>

<p>This one is quite controversial, especially when compared to previously mentioned features. There are many programmers considering implicit interfaces as potentially unsafe, for example by weakening type safety in static typing by creating risk of accidential interface implementation. There are many examples provided to support this thesis, but actually I found most of them is result of bad design decision and broken conventions, or simply explicit interfaces are not solution to presented problems either.</p>

<p>So in what cases implicit interfaces could be usefull:</p>

<ol>
<li>Just like in case of monkey patching it&rsquo;s a way to provide additional features to existing types, that we have no access to. This may be especially usefull to patching holes in stdlib implementations (no matter how hard you try, you never foresee all of them). Example: lack of IParseable interface in .NET common lib.</li>
<li>It makes easier to work with various external libs, minimazing need of constructing adapters to bypass incompatibilities.</li>
<li>It also makes very simple to create mocks.</li>
</ol>


<p>They also encourage some good practices when working with interfaces. Since they&rsquo;re implicit, you will naturally try to keep them as simple and decoupled as possible. It&rsquo;s easy to maintain implementation of interface having dozens of methods, when they are explicit, it&rsquo;s harder to do so in implicit way. And since the purpose of interface is to abstract and describe contract between parts of your application, interfaces with numerous methods are often symphoms of a God Objects in your code. On the other hand it&rsquo;s easier to create a class implementing dozens of interfaces, when they&rsquo;re implicit rather than explicit. This also promotes decoupling.</p>

<h2>Great absent</h2>

<p>Actually I didn&rsquo;t mentioned some of most core features, which benefit all statically typed languages, such as types as first-class citizens, type inference or language macros/templates. The reason of this is, that those thing are well known and more or less present in many of the popular languages we have today. My goal was to describe some potential advantages of powerfull, yet less known features we may see with hope they would be more often met and used in the future.</p>
]]></content>
  </entry>
  
</feed>
